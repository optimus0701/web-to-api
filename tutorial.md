# H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng WebAI-to-API Trong D·ª± √Ån Kh√°c

## üìã M·ª•c L·ª•c
- [Gi·ªõi Thi·ªáu](#gi·ªõi-thi·ªáu)
- [C√†i ƒê·∫∑t v√† C·∫•u H√¨nh](#c√†i-ƒë·∫∑t-v√†-c·∫•u-h√¨nh)
- [T√≠ch H·ª£p API](#t√≠ch-h·ª£p-api)
- [API Endpoints](#api-endpoints)
- [V√≠ D·ª• T√≠ch H·ª£p](#v√≠-d·ª•-t√≠ch-h·ª£p)
- [Troubleshooting](#troubleshooting)

---

## Gi·ªõi Thi·ªáu

**WebAI-to-API** l√† m·ªôt web server ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng FastAPI cho ph√©p b·∫°n bi·∫øn c√°c LLM d·ª±a tr√™n tr√¨nh duy·ªát (nh∆∞ Gemini) th√†nh c√°c API endpoint c·ª•c b·ªô. D·ª± √°n h·ªó tr·ª£ hai ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông:

1. **WebAI Server** (Ch√≠nh): K·∫øt n·ªëi v·ªõi giao di·ªán web Gemini th√¥ng qua cookies c·ªßa tr√¨nh duy·ªát
2. **gpt4free Server** (D·ª± ph√≤ng): H·ªó tr·ª£ nhi·ªÅu LLM kh√°c nhau (ChatGPT, Claude, DeepSeek, v.v.)

### C√°c T√≠nh NƒÉng Ch√≠nh

- üöÄ **OpenAI-compatible API**: T∆∞∆°ng th√≠ch v·ªõi ƒë·ªãnh d·∫°ng API c·ªßa OpenAI
- üîÑ **H·ªó tr·ª£ conversation history**: Duy tr√¨ ng·ªØ c·∫£nh h·ªôi tho·∫°i
- üåê **Nhi·ªÅu endpoints linh ho·∫°t**: `/gemini`, `/gemini-chat`, `/translate`, `/v1/chat/completions`
- üéØ **H·ªó tr·ª£ nhi·ªÅu models**: gemini-3.0-pro, gemini-2.5-pro, gemini-2.5-flash

---

## C√†i ƒê·∫∑t v√† C·∫•u H√¨nh

### B∆∞·ªõc 1: Clone v√† C√†i ƒê·∫∑t Dependencies

```bash
# Clone repository
git clone https://github.com/Amm1rr/WebAI-to-API.git
cd WebAI-to-API

# C√†i ƒë·∫∑t dependencies b·∫±ng Poetry
poetry install

# Ho·∫∑c s·ª≠ d·ª•ng pip
pip install -r requirements.txt
```

### B∆∞·ªõc 2: T·∫°o File C·∫•u H√¨nh

```bash
# T·∫°o file config.conf t·ª´ template
cp config.conf.example config.conf
```

### B∆∞·ªõc 3: Ch·ªânh S·ª≠a File `config.conf`

```ini
[AI]
# AI service m·∫∑c ƒë·ªãnh
default_ai = gemini

# Model m·∫∑c ƒë·ªãnh cho Gemini
# T√πy ch·ªçn: gemini-3.0-pro, gemini-2.5-pro, gemini-2.5-flash
default_model_gemini = gemini-2.5-flash

# Gemini cookies (ƒë·ªÉ tr·ªëng n·∫øu mu·ªën t·ª± ƒë·ªông l·∫•y t·ª´ tr√¨nh duy·ªát)
gemini_cookie_1psid =
gemini_cookie_1psidts =

[EnabledAI]
# B·∫≠t/t·∫Øt AI services
gemini = true

[Browser]
# Tr√¨nh duy·ªát m·∫∑c ƒë·ªãnh ƒë·ªÉ l·∫•y cookies
# T√πy ch·ªçn: firefox, brave, chrome, edge, safari
name = firefox

[Proxy]
# Proxy t√πy ch·ªçn cho k·∫øt n·ªëi Gemini (ƒë·ªÉ kh·∫Øc ph·ª•c l·ªói 403)
http_proxy =
```

### B∆∞·ªõc 4: Ch·∫°y Server

```bash
# Ch·∫°y server (m·∫∑c ƒë·ªãnh t·∫°i localhost:6969)
poetry run python src/run.py

# Ch·∫°y v·ªõi c√°c t√πy ch·ªçn t√πy ch·ªânh
poetry run python src/run.py --host 0.0.0.0 --port 8080
```

**üéâ Server s·∫Ω kh·ªüi ƒë·ªông t·∫°i:** `http://localhost:6969`

**üìö Xem API docs t·∫°i:** `http://localhost:6969/docs`

---

## T√≠ch H·ª£p API

### Th√¥ng Tin K·∫øt N·ªëi

Sau khi ch·∫°y server th√†nh c√¥ng, b·∫°n c√≥ th·ªÉ t√≠ch h·ª£p API v√†o d·ª± √°n c·ªßa m√¨nh:

```
Base URL: http://localhost:6969
```

### C√°c Models ƒê∆∞·ª£c H·ªó Tr·ª£

| Model | M√¥ T·∫£ |
|-------|-------|
| `gemini-3.0-pro` | Model m·∫°nh nh·∫•t v√† m·ªõi nh·∫•t |
| `gemini-2.5-pro` | Model l√Ω lu·∫≠n n√¢ng cao |
| `gemini-2.5-flash` | Model nhanh v√† hi·ªáu qu·∫£ (m·∫∑c ƒë·ªãnh) |

---

## API Endpoints

### 1. `/v1/chat/completions` (Khuy·∫øn Ngh·ªã)

**OpenAI-compatible endpoint** - T·ªët nh·∫•t cho t√≠ch h·ª£p v√†o c√°c ·ª©ng d·ª•ng hi·ªán c√≥.

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ H·ªó tr·ª£ system prompts
- ‚úÖ Duy tr√¨ conversation history
- ‚úÖ T∆∞∆°ng th√≠ch v·ªõi OpenAI API format
- ‚úÖ H·ªó tr·ª£ streaming (t√πy ch·ªçn)

**Request:**

```http
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "gemini-2.5-flash",
  "messages": [
    {
      "role": "system",
      "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch."
    },
    {
      "role": "user",
      "content": "Python l√† g√¨?"
    }
  ],
  "stream": false
}
```

**Response:**

```json
{
  "id": "chatcmpl-1704088800",
  "object": "chat.completion",
  "created": 1704088800,
  "model": "gemini-2.5-flash",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0
  }
}
```

---

### 2. `/gemini`

T·∫°o m·ªôt cu·ªôc h·ªôi tho·∫°i m·ªõi v·ªõi LLM. M·ªói request t·∫°o m·ªôt **session m·ªõi**.

**Request:**

```http
POST /gemini
Content-Type: application/json

{
  "message": "Xin ch√†o!",
  "model": "gemini-2.5-flash",
  "files": []
}
```

**Response:**

```json
{
  "response": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
}
```

---

### 3. `/gemini-chat`

Ti·∫øp t·ª•c cu·ªôc h·ªôi tho·∫°i v·ªõi LLM **kh√¥ng t·∫°o session m·ªõi**. Duy tr√¨ ng·ªØ c·∫£nh gi·ªØa c√°c messages.

**Request:**

```http
POST /gemini-chat
Content-Type: application/json

{
  "message": "Ti·∫øp t·ª•c c√¢u chuy·ªán tr∆∞·ªõc ƒë√≥",
  "model": "gemini-2.5-flash",
  "files": []
}
```

**Response:**

```json
{
  "response": "ƒê∆∞·ª£c, t√¥i s·∫Ω ti·∫øp t·ª•c..."
}
```

---

### 4. `/translate`

ƒê∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ t√≠ch h·ª£p v·ªõi extension [Translate It!](https://github.com/iSegaro/Translate-It). **Duy tr√¨ session** gi·ªëng nh∆∞ `/gemini-chat`.

**Request:**

```http
POST /translate
Content-Type: application/json

{
  "message": "Translate this to Vietnamese: Hello world",
  "model": "gemini-2.5-flash"
}
```

---

### 5. `/v1beta/models/{model}` 

**Google Generative AI v1beta API** compatible endpoint.

**Request:**

```http
POST /v1beta/models/gemini-2.5-flash
Content-Type: application/json

{
  "contents": [
    {
      "parts": [
        {
          "text": "Gi·∫£i th√≠ch v·ªÅ AI"
        }
      ]
    }
  ]
}
```

---

## V√≠ D·ª• T√≠ch H·ª£p

### Python (v·ªõi `requests`)

```python
import requests

# Base URL c·ªßa WebAI-to-API server
BASE_URL = "http://localhost:6969"

def chat_with_gemini(message, model="gemini-2.5-flash"):
    """
    G·ª≠i message t·ªõi Gemini API
    
    Args:
        message: N·ªôi dung tin nh·∫Øn
        model: Model s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh: gemini-2.5-flash)
    
    Returns:
        Response text t·ª´ Gemini
    """
    url = f"{BASE_URL}/v1/chat/completions"
    
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": message
            }
        ]
    }
    
    response = requests.post(url, json=payload)
    response.raise_for_status()
    
    result = response.json()
    return result["choices"][0]["message"]["content"]

# S·ª≠ d·ª•ng
try:
    answer = chat_with_gemini("Cho t√¥i bi·∫øt v·ªÅ Python?")
    print(f"Gemini: {answer}")
except Exception as e:
    print(f"L·ªói: {e}")
```

### Python v·ªõi Conversation History

```python
import requests

BASE_URL = "http://localhost:6969"

class GeminiConversation:
    """Qu·∫£n l√Ω cu·ªôc h·ªôi tho·∫°i v·ªõi Gemini"""
    
    def __init__(self, system_prompt=None, model="gemini-2.5-flash"):
        self.model = model
        self.messages = []
        
        if system_prompt:
            self.messages.append({
                "role": "system",
                "content": system_prompt
            })
    
    def send_message(self, user_message):
        """G·ª≠i message v√† nh·∫≠n response"""
        # Th√™m user message v√†o l·ªãch s·ª≠
        self.messages.append({
            "role": "user",
            "content": user_message
        })
        
        # G·ª≠i request
        response = requests.post(
            f"{BASE_URL}/v1/chat/completions",
            json={
                "model": self.model,
                "messages": self.messages
            }
        )
        response.raise_for_status()
        
        # L·∫•y response
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        
        # Th√™m assistant response v√†o l·ªãch s·ª≠
        self.messages.append({
            "role": "assistant",
            "content": assistant_message
        })
        
        return assistant_message
    
    def clear_history(self):
        """X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i (gi·ªØ system prompt)"""
        system_messages = [m for m in self.messages if m["role"] == "system"]
        self.messages = system_messages

# S·ª≠ d·ª•ng
conversation = GeminiConversation(
    system_prompt="B·∫°n l√† m·ªôt gi√°o vi√™n l·∫≠p tr√¨nh Python chuy√™n nghi·ªáp."
)

# H·ªôi tho·∫°i li√™n t·ª•c
print(conversation.send_message("Python l√† g√¨?"))
print(conversation.send_message("N√≥ c√≥ d·ªÖ h·ªçc kh√¥ng?"))
print(conversation.send_message("Cho t√¥i m·ªôt v√≠ d·ª• code Python ƒë∆°n gi·∫£n"))
```

---

### JavaScript/Node.js (v·ªõi `axios`)

```javascript
const axios = require('axios');

const BASE_URL = 'http://localhost:6969';

/**
 * G·ª≠i message t·ªõi Gemini API
 * @param {string} message - N·ªôi dung tin nh·∫Øn
 * @param {string} model - Model s·ª≠ d·ª•ng
 * @returns {Promise<string>} Response t·ª´ Gemini
 */
async function chatWithGemini(message, model = 'gemini-2.5-flash') {
  try {
    const response = await axios.post(`${BASE_URL}/v1/chat/completions`, {
      model: model,
      messages: [
        {
          role: 'user',
          content: message
        }
      ]
    });
    
    return response.data.choices[0].message.content;
  } catch (error) {
    console.error('L·ªói:', error.message);
    throw error;
  }
}

// S·ª≠ d·ª•ng
(async () => {
  try {
    const answer = await chatWithGemini('Gi·∫£i th√≠ch v·ªÅ JavaScript?');
    console.log(`Gemini: ${answer}`);
  } catch (error) {
    console.error('C√≥ l·ªói x·∫£y ra:', error);
  }
})();
```

### JavaScript v·ªõi Conversation History

```javascript
const axios = require('axios');

const BASE_URL = 'http://localhost:6969';

class GeminiConversation {
  constructor(systemPrompt = null, model = 'gemini-2.5-flash') {
    this.model = model;
    this.messages = [];
    
    if (systemPrompt) {
      this.messages.push({
        role: 'system',
        content: systemPrompt
      });
    }
  }
  
  async sendMessage(userMessage) {
    // Th√™m user message
    this.messages.push({
      role: 'user',
      content: userMessage
    });
    
    // G·ª≠i request
    const response = await axios.post(`${BASE_URL}/v1/chat/completions`, {
      model: this.model,
      messages: this.messages
    });
    
    const assistantMessage = response.data.choices[0].message.content;
    
    // Th√™m assistant response v√†o l·ªãch s·ª≠
    this.messages.push({
      role: 'assistant',
      content: assistantMessage
    });
    
    return assistantMessage;
  }
  
  clearHistory() {
    const systemMessages = this.messages.filter(m => m.role === 'system');
    this.messages = systemMessages;
  }
}

// S·ª≠ d·ª•ng
(async () => {
  const conversation = new GeminiConversation(
    'B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh.'
  );
  
  console.log(await conversation.sendMessage('Xin ch√†o!'));
  console.log(await conversation.sendMessage('B·∫°n c√≥ th·ªÉ gi√∫p g√¨ cho t√¥i?'));
})();
```

---

### TypeScript (Discord Bot Example)

```typescript
import axios from 'axios';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatCompletionRequest {
  model: string;
  messages: ChatMessage[];
  stream?: boolean;
}

interface ChatCompletionResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

class GeminiClient {
  private baseUrl: string;
  private defaultModel: string;

  constructor(baseUrl: string = 'http://localhost:6969', defaultModel: string = 'gemini-2.5-flash') {
    this.baseUrl = baseUrl;
    this.defaultModel = defaultModel;
  }

  async chat(messages: ChatMessage[], model?: string): Promise<string> {
    const requestData: ChatCompletionRequest = {
      model: model || this.defaultModel,
      messages: messages,
      stream: false
    };

    try {
      const response = await axios.post<ChatCompletionResponse>(
        `${this.baseUrl}/v1/chat/completions`,
        requestData,
        {
          headers: {
            'Content-Type': 'application/json'
          }
        }
      );

      return response.data.choices[0].message.content;
    } catch (error) {
      if (axios.isAxiosError(error)) {
        throw new Error(`API Error: ${error.response?.data || error.message}`);
      }
      throw error;
    }
  }

  async simpleChat(message: string, systemPrompt?: string): Promise<string> {
    const messages: ChatMessage[] = [];
    
    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }
    
    messages.push({ role: 'user', content: message });
    
    return this.chat(messages);
  }
}

// S·ª≠ d·ª•ng trong Discord Bot
import { Client, GatewayIntentBits, Message } from 'discord.js';

const client = new Client({
  intents: [
    GatewayIntentBits.Guilds,
    GatewayIntentBits.GuildMessages,
    GatewayIntentBits.MessageContent
  ]
});

const gemini = new GeminiClient();

client.on('messageCreate', async (message: Message) => {
  // B·ªè qua tin nh·∫Øn t·ª´ bot
  if (message.author.bot) return;
  
  // Ch·ªâ ph·∫£n h·ªìi khi ƒë∆∞·ª£c mention
  if (message.mentions.has(client.user!)) {
    try {
      const userMessage = message.content.replace(`<@${client.user!.id}>`, '').trim();
      
      const response = await gemini.simpleChat(
        userMessage,
        'B·∫°n l√† m·ªôt bot Discord th√¥ng minh v√† h·ªØu √≠ch.'
      );
      
      await message.reply(response);
    } catch (error) {
      console.error('Error:', error);
      await message.reply('Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n.');
    }
  }
});

client.login('YOUR_DISCORD_BOT_TOKEN');
```

---

### cURL Examples

#### Request ƒë∆°n gi·∫£n

```bash
curl -X POST http://localhost:6969/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {
        "role": "user",
        "content": "Xin ch√†o!"
      }
    ]
  }'
```

#### Request v·ªõi system prompt v√† conversation history

```bash
curl -X POST http://localhost:6969/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-pro",
    "messages": [
      {
        "role": "system",
        "content": "B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ AI."
      },
      {
        "role": "user",
        "content": "AI l√† g√¨?"
      },
      {
        "role": "assistant",
        "content": "AI (Artificial Intelligence) l√† tr√≠ tu·ªá nh√¢n t·∫°o..."
      },
      {
        "role": "user",
        "content": "N√≥ c√≥ ·ª©ng d·ª•ng g√¨?"
      }
    ]
  }'
```

---

### PHP Example

```php
<?php

class GeminiClient {
    private $baseUrl;
    private $defaultModel;
    
    public function __construct($baseUrl = 'http://localhost:6969', $defaultModel = 'gemini-2.5-flash') {
        $this->baseUrl = $baseUrl;
        $this->defaultModel = $defaultModel;
    }
    
    public function chat($messages, $model = null) {
        $url = $this->baseUrl . '/v1/chat/completions';
        
        $data = [
            'model' => $model ?? $this->defaultModel,
            'messages' => $messages
        ];
        
        $ch = curl_init($url);
        curl_setopt($ch, CURLOPT_POST, true);
        curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));
        curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
        curl_setopt($ch, CURLOPT_HTTPHEADER, [
            'Content-Type: application/json'
        ]);
        
        $response = curl_exec($ch);
        $httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
        curl_close($ch);
        
        if ($httpCode !== 200) {
            throw new Exception("API Error: HTTP $httpCode");
        }
        
        $result = json_decode($response, true);
        return $result['choices'][0]['message']['content'];
    }
    
    public function simpleChat($message, $systemPrompt = null) {
        $messages = [];
        
        if ($systemPrompt) {
            $messages[] = [
                'role' => 'system',
                'content' => $systemPrompt
            ];
        }
        
        $messages[] = [
            'role' => 'user',
            'content' => $message
        ];
        
        return $this->chat($messages);
    }
}

// S·ª≠ d·ª•ng
try {
    $gemini = new GeminiClient();
    $response = $gemini->simpleChat(
        'Gi·∫£i th√≠ch v·ªÅ PHP?',
        'B·∫°n l√† m·ªôt chuy√™n gia l·∫≠p tr√¨nh PHP.'
    );
    
    echo "Gemini: " . $response . "\n";
} catch (Exception $e) {
    echo "L·ªói: " . $e->getMessage() . "\n";
}
?>
```

---

## Troubleshooting

### 1. L·ªói "Gemini client is not initialized"

**Nguy√™n nh√¢n:** Server kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini client (th∆∞·ªùng do thi·∫øu cookies ho·∫∑c cookies kh√¥ng h·ª£p l·ªá).

**Gi·∫£i ph√°p:**

```bash
# Option 1: ƒê·ªÉ server t·ª± ƒë·ªông l·∫•y cookies t·ª´ tr√¨nh duy·ªát
# ƒê·∫£m b·∫£o b·∫°n ƒë√£ ƒëƒÉng nh·∫≠p Gemini tr√™n tr√¨nh duy·ªát (Firefox, Chrome, v.v.)
# Trong config.conf:
[Browser]
name = firefox  # ho·∫∑c chrome, brave, edge, safari

# Option 2: Cung c·∫•p cookies th·ªß c√¥ng
# L·∫•y cookies __Secure-1PSID v√† __Secure-1PSIDTS t·ª´ tr√¨nh duy·ªát
# Trong config.conf:
[AI]
gemini_cookie_1psid = YOUR_1PSID_COOKIE
gemini_cookie_1psidts = YOUR_1PSIDTS_COOKIE
```

### 2. L·ªói 403 khi k·∫øt n·ªëi Gemini

**Nguy√™n nh√¢n:** IP b·ªã ch·∫∑n ho·∫∑c v·ªã tr√≠ ƒë·ªãa l√Ω kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.

**Gi·∫£i ph√°p:** S·ª≠ d·ª•ng proxy

```ini
# Trong config.conf:
[Proxy]
http_proxy = http://127.0.0.1:2334
```

### 3. L·ªói "Connection refused"

**Nguy√™n nh√¢n:** Server ch∆∞a ch·∫°y ho·∫∑c ƒëang ch·∫°y ·ªü port kh√°c.

**Gi·∫£i ph√°p:**

```bash
# Ki·ªÉm tra server c√≥ ƒëang ch·∫°y kh√¥ng
# ƒê·∫£m b·∫£o base URL ch√≠nh x√°c, v√≠ d·ª•:
http://localhost:6969  # port m·∫∑c ƒë·ªãnh
```

### 4. Response ch·∫≠m ho·∫∑c timeout

**Nguy√™n nh√¢n:** Model ph·ª©c t·∫°p ho·∫∑c message qu√° d√†i.

**Gi·∫£i ph√°p:**

```python
# S·ª≠ d·ª•ng model nhanh h∆°n
payload = {
    "model": "gemini-2.5-flash",  # Thay v√¨ gemini-3.0-pro
    "messages": [...]
}

# Ho·∫∑c tƒÉng timeout trong client
response = requests.post(url, json=payload, timeout=60)  # 60 gi√¢y
```

### 5. Chuy·ªÉn ƒë·ªïi gi·ªØa WebAI v√† gpt4free

Khi server ƒëang ch·∫°y, b·∫°n c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa c√°c ch·∫ø ƒë·ªô:

```
Nh·∫•n '1' + Enter: Chuy·ªÉn sang WebAI mode (nhanh h∆°n, d√πng Gemini)
Nh·∫•n '2' + Enter: Chuy·ªÉn sang gpt4free mode (nhi·ªÅu model h∆°n)
Ctrl+C: Tho√°t server
```

### 6. L·ªói khi s·ª≠ d·ª•ng gpt4free mode

**L·ªói:** `ProviderNotFoundError`

**Gi·∫£i ph√°p:** Khi s·ª≠ d·ª•ng gpt4free, b·∫°n c·∫ßn ch·ªâ ƒë·ªãnh provider h·ª£p l·ªá, kh√¥ng ph·∫£i model name.

```bash
# Ki·ªÉm tra danh s√°ch providers c√≥ s·∫µn:
curl http://localhost:6969/v1/providers

# S·ª≠ d·ª•ng ƒë√∫ng provider trong request
```

---

## Best Practices

### 1. Qu·∫£n l√Ω Session hi·ªáu qu·∫£

- S·ª≠ d·ª•ng `/v1/chat/completions` cho h·∫ßu h·∫øt c√°c use cases
- S·ª≠ d·ª•ng `/gemini` khi c·∫ßn session m·ªõi cho m·ªói request
- S·ª≠ d·ª•ng `/gemini-chat` khi c·∫ßn duy tr√¨ context gi·ªØa c√°c request

### 2. Ch·ªçn Model ph√π h·ª£p

| Use Case | Recommended Model |
|----------|------------------|
| Ph·∫£n h·ªìi nhanh, chat ƒë∆°n gi·∫£n | `gemini-2.5-flash` |
| Ph√¢n t√≠ch ph·ª©c t·∫°p, reasoning | `gemini-2.5-pro` |
| Tasks quan tr·ªçng, latest features | `gemini-3.0-pro` |

### 3. Error Handling

Lu√¥n implement error handling trong code:

```python
try:
    response = requests.post(url, json=payload, timeout=30)
    response.raise_for_status()
    result = response.json()
except requests.exceptions.Timeout:
    print("Request timeout - th·ª≠ l·∫°i sau")
except requests.exceptions.HTTPError as e:
    print(f"HTTP error: {e}")
except requests.exceptions.RequestException as e:
    print(f"Request error: {e}")
```

### 4. Rate Limiting

Tr√°nh spam requests:

```python
import time

def chat_with_rate_limit(message, min_delay=1.0):
    """Th√™m delay gi·ªØa c√°c requests"""
    response = chat_with_gemini(message)
    time.sleep(min_delay)
    return response
```

---

## T√†i Li·ªáu Tham Kh·∫£o

- **Repository ch√≠nh:** https://github.com/Amm1rr/WebAI-to-API
- **API Documentation:** http://localhost:6969/docs (khi server ƒëang ch·∫°y)
- **gpt4free Documentation:** https://github.com/xtekky/gpt4free

---

## K·∫øt Lu·∫≠n

WebAI-to-API cung c·∫•p m·ªôt c√°ch ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£ ƒë·ªÉ t√≠ch h·ª£p Gemini AI v√†o ·ª©ng d·ª•ng c·ªßa b·∫°n th√¥ng qua API RESTful. V·ªõi h∆∞·ªõng d·∫´n n√†y, b·∫°n c√≥ th·ªÉ:

- ‚úÖ C√†i ƒë·∫∑t v√† c·∫•u h√¨nh server
- ‚úÖ T√≠ch h·ª£p API v√†o b·∫•t k·ª≥ ng√¥n ng·ªØ l·∫≠p tr√¨nh n√†o
- ‚úÖ S·ª≠ d·ª•ng conversation history v√† system prompts
- ‚úÖ X·ª≠ l√Ω l·ªói v√† troubleshooting hi·ªáu qu·∫£

**Ch√∫c b·∫°n coding vui v·∫ª! üöÄ**
