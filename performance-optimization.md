# T·ªëi ∆Øu Performance cho WebAI-to-API

H∆∞·ªõng d·∫´n c·∫£i thi·ªán t·ªëc ƒë·ªô response v√† kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ ch·∫≠m.

---

## üêå Nguy√™n Nh√¢n Response Ch·∫≠m

### 1. **Model Selection**
- `gemini-3.0-pro`: Ch·∫≠m nh·∫•t nh∆∞ng m·∫°nh nh·∫•t
- `gemini-2.5-pro`: Trung b√¨nh
- `gemini-2.5-flash`: Nhanh nh·∫•t ‚ö° (khuy·∫øn ngh·ªã)

### 2. **Network Issues**
- K·∫øt n·ªëi t·ªõi Gemini servers ch·∫≠m
- Cookies h·∫øt h·∫°n
- B·ªã rate limit

### 3. **Message Length**
- Conversation history qu√° d√†i
- Input text qu√° l·ªõn

### 4. **Server Configuration**
- Ch·∫°y development mode thay v√¨ production
- Kh√¥ng ƒë·ªß workers

---

## ‚ö° Gi·∫£i Ph√°p T·ªëi ∆Øu

### 1Ô∏è‚É£ S·ª≠ D·ª•ng Model Nhanh Nh·∫•t

**Trong `config.conf`:**
```ini
[AI]
default_model_gemini = gemini-2.5-flash
```

**Trong API request:**
```json
{
  "model": "gemini-2.5-flash",
  "messages": [...]
}
```

**K·∫øt qu·∫£:** Response nhanh g·∫•p 2-3 l·∫ßn so v·ªõi `gemini-3.0-pro`

---

### 2Ô∏è‚É£ C·∫•u H√¨nh Proxy (N·∫øu B·ªã Ch·∫∑n)

N·∫øu b·ªã l·ªói 403 ho·∫∑c connection timeout:

**Trong `config.conf`:**
```ini
[Proxy]
http_proxy = http://127.0.0.1:7890
```

**S·ª≠ d·ª•ng proxy nhanh:**
- Cloudflare WARP
- Local proxy (v2ray, clash)

---

### 3Ô∏è‚É£ Gi·ªõi H·∫°n Conversation History

**‚ùå Kh√¥ng n√™n:**
```json
{
  "model": "gemini-2.5-flash",
  "messages": [
    // 50+ messages ·ªü ƒë√¢y...
  ]
}
```

**‚úÖ N√™n:**
```json
{
  "model": "gemini-2.5-flash",
  "messages": [
    {"role": "system", "content": "..."},
    // Ch·ªâ gi·ªØ 5-10 messages g·∫ßn nh·∫•t
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."},
    {"role": "user", "content": "C√¢u h·ªèi m·ªõi"}
  ]
}
```

**Code Python ƒë·ªÉ gi·ªõi h·∫°n history:**
```python
def limit_history(messages, max_messages=10):
    """Gi·ªØ system prompt + N messages g·∫ßn nh·∫•t"""
    system_messages = [m for m in messages if m["role"] == "system"]
    other_messages = [m for m in messages if m["role"] != "system"]
    
    # Gi·ªØ N messages cu·ªëi
    limited_messages = other_messages[-max_messages:]
    
    return system_messages + limited_messages

# S·ª≠ d·ª•ng
messages = limit_history(all_messages, max_messages=8)
```

---

### 4Ô∏è‚É£ Ch·∫°y Production Mode v·ªõi Multiple Workers

**V·ªõi Docker:**

**File `.env`:**
```env
ENVIRONMENT=production
```

**Ch·∫°y:**
```bash
docker compose -f docker-compose.yml up -d
```

**K·∫øt qu·∫£:** Server ch·∫°y v·ªõi 4 workers, x·ª≠ l√Ω ƒë·ªìng th·ªùi nhi·ªÅu requests.

---

**Ch·∫°y Local:**

```bash
# Thay v√¨: poetry run python src/run.py
# Ch·∫°y v·ªõi uvicorn tr·ª±c ti·∫øp:

cd src
uvicorn app.main:app --host 0.0.0.0 --port 6969 --workers 4
```

---

### 5Ô∏è‚É£ C·∫≠p Nh·∫≠t Cookies Gemini

Cookies c≈© c√≥ th·ªÉ l√†m ch·∫≠m ho·∫∑c g√¢y l·ªói.

**Option 1: T·ª± ƒë·ªông l·∫•y t·ª´ browser**

```ini
# config.conf
[AI]
gemini_cookie_1psid =
gemini_cookie_1psidts =

[Browser]
name = firefox  # ho·∫∑c chrome
```

Server s·∫Ω t·ª± ƒë·ªông l·∫•y cookies m·ªõi nh·∫•t.

**Option 2: C·∫≠p nh·∫≠t th·ªß c√¥ng**

1. M·ªü https://gemini.google.com
2. ƒêƒÉng nh·∫≠p l·∫°i
3. L·∫•y cookies m·ªõi (F12 ‚Üí Application ‚Üí Cookies)
4. C·∫≠p nh·∫≠t v√†o `config.conf`
5. Restart server

---

### 6Ô∏è‚É£ S·ª≠ D·ª•ng Streaming (Cho Response L·ªõn)

**Request v·ªõi streaming:**
```json
{
  "model": "gemini-2.5-flash",
  "messages": [...],
  "stream": true
}
```

**L·ª£i √≠ch:**
- User th·∫•y response ngay l·∫≠p t·ª©c
- Kh√¥ng ph·∫£i ƒë·ª£i to√†n b·ªô response

**Python client v·ªõi streaming:**
```python
import requests

def chat_stream(messages):
    url = "http://localhost:6969/v1/chat/completions"
    
    payload = {
        "model": "gemini-2.5-flash",
        "messages": messages,
        "stream": True
    }
    
    with requests.post(url, json=payload, stream=True) as response:
        for chunk in response.iter_content(chunk_size=None):
            if chunk:
                print(chunk.decode(), end='', flush=True)

# S·ª≠ d·ª•ng
chat_stream([
    {"role": "user", "content": "Vi·∫øt m·ªôt c√¢u chuy·ªán d√†i"}
])
```

---

### 7Ô∏è‚É£ T·ªëi ∆Øu httpx Connection Pool

**T·∫°o file `src/app/config_httpx.py`:**

```python
import httpx

# C·∫•u h√¨nh connection pool
HTTPX_LIMITS = httpx.Limits(
    max_keepalive_connections=20,
    max_connections=100,
    keepalive_expiry=30.0
)

HTTPX_TIMEOUT = httpx.Timeout(
    timeout=60.0,  # T·ªïng timeout
    connect=10.0,  # Connect timeout
    read=50.0,     # Read timeout
    write=10.0     # Write timeout
)
```

---

### 8Ô∏è‚É£ Cache Response (Cho Repeated Queries)

**S·ª≠ d·ª•ng Redis cache:**

```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_cache_key(messages):
    """T·∫°o cache key t·ª´ messages"""
    content = json.dumps(messages, sort_keys=True)
    return hashlib.md5(content.encode()).hexdigest()

def chat_with_cache(messages):
    cache_key = get_cache_key(messages)
    
    # Ki·ªÉm tra cache
    cached = redis_client.get(cache_key)
    if cached:
        print("üì¶ From cache")
        return json.loads(cached)
    
    # G·ªçi API
    response = requests.post(
        "http://localhost:6969/v1/chat/completions",
        json={"model": "gemini-2.5-flash", "messages": messages}
    ).json()
    
    # L∆∞u v√†o cache (1 gi·ªù)
    redis_client.setex(cache_key, 3600, json.dumps(response))
    
    return response
```

---

### 9Ô∏è‚É£ S·ª≠ D·ª•ng gpt4free Mode (Fallback)

N·∫øu Gemini qu√° ch·∫≠m, chuy·ªÉn sang gpt4free mode:

**Khi server ch·∫°y:**
```
Press '2' then Enter
```

**Ho·∫∑c c·∫•u h√¨nh:**
```ini
# config.conf
[EnabledAI]
gemini = false  # T·∫Øt Gemini, d√πng g4f
```

**API request:**
```json
{
  "model": "DDG",  // S·ª≠ d·ª•ng provider thay v√¨ model
  "messages": [...]
}
```

---

### üîü Load Balancing (Advanced)

**Ch·∫°y nhi·ªÅu instances:**

```bash
# Instance 1
uvicorn app.main:app --host 0.0.0.0 --port 6969

# Instance 2
uvicorn app.main:app --host 0.0.0.0 --port 6970

# Instance 3
uvicorn app.main:app --host 0.0.0.0 --port 6971
```

**Nginx load balancer:**

```nginx
upstream webai {
    server 127.0.0.1:6969;
    server 127.0.0.1:6970;
    server 127.0.0.1:6971;
}

server {
    listen 80;
    
    location / {
        proxy_pass http://webai;
    }
}
```

---

## üìä Benchmark Results

| Configuration | Avg Response Time |
|---------------|-------------------|
| gemini-3.0-pro (default) | ~8-12s |
| gemini-2.5-pro | ~5-8s |
| gemini-2.5-flash | ~2-4s ‚ö° |
| gemini-2.5-flash + proxy | ~1.5-3s ‚ö°‚ö° |
| gemini-2.5-flash + streaming | Instant UI response ‚ö°‚ö°‚ö° |

---

## ‚úÖ Recommended Configuration

**File `config.conf` t·ªëi ∆∞u:**

```ini
[AI]
# S·ª≠ d·ª•ng model nhanh nh·∫•t
default_ai = gemini
default_model_gemini = gemini-2.5-flash

# ƒê·ªÉ tr·ªëng ƒë·ªÉ auto-detect cookies
gemini_cookie_1psid =
gemini_cookie_1psidts =

[EnabledAI]
gemini = true

[Browser]
# Browser b·∫°n ƒë√£ ƒëƒÉng nh·∫≠p Gemini
name = chrome

[Proxy]
# S·ª≠ d·ª•ng proxy n·∫øu c·∫ßn (t√πy ch·ªçn)
http_proxy =
```

---

## üöÄ Quick Wins

### Thay ƒê·ªïi Ngay L·∫≠p T·ª©c:

1. **ƒê·ªïi model sang flash:**
   ```json
   {"model": "gemini-2.5-flash"}
   ```

2. **Gi·ªõi h·∫°n conversation history:**
   ```python
   messages = messages[-8:]  # Ch·ªâ gi·ªØ 8 messages cu·ªëi
   ```

3. **Ch·∫°y production mode:**
   ```bash
   cd src
   uvicorn app.main:app --host 0.0.0.0 --port 6969 --workers 4
   ```

### K·∫øt Qu·∫£:
- ‚ö° Response nhanh g·∫•p 2-4 l·∫ßn
- üí™ X·ª≠ l√Ω ƒë∆∞·ª£c nhi·ªÅu requests h∆°n
- üéØ ·ªîn ƒë·ªãnh h∆°n

---

## üîç Troubleshooting Performance

### Ki·ªÉm Tra Response Time

**V·ªõi curl:**
```bash
time curl -X POST http://localhost:6969/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "Hi"}]}'
```

**V·ªõi Python:**
```python
import time
import requests

start = time.time()
response = requests.post(
    "http://localhost:6969/v1/chat/completions",
    json={
        "model": "gemini-2.5-flash",
        "messages": [{"role": "user", "content": "Hello"}]
    }
)
elapsed = time.time() - start

print(f"Response time: {elapsed:.2f}s")
```

### Xem Server Logs

```bash
# Local
poetry run python src/run.py

# Docker
docker logs -f web_ai_server
```

T√¨m c√°c d√≤ng:
- `ERROR` - L·ªói
- `WARNING` - C·∫£nh b√°o
- `INFO` - Th√¥ng tin

---

## üìà Monitoring Performance

**Script monitor response time:**

```python
import requests
import time
from collections import deque

class PerformanceMonitor:
    def __init__(self, window_size=10):
        self.response_times = deque(maxlen=window_size)
    
    def chat(self, message):
        start = time.time()
        
        response = requests.post(
            "http://localhost:6969/v1/chat/completions",
            json={
                "model": "gemini-2.5-flash",
                "messages": [{"role": "user", "content": message}]
            }
        )
        
        elapsed = time.time() - start
        self.response_times.append(elapsed)
        
        print(f"Response time: {elapsed:.2f}s")
        print(f"Average (last {len(self.response_times)}): {self.avg():.2f}s")
        
        return response.json()
    
    def avg(self):
        if not self.response_times:
            return 0
        return sum(self.response_times) / len(self.response_times)

# S·ª≠ d·ª•ng
monitor = PerformanceMonitor()
monitor.chat("Hello")
monitor.chat("How are you?")
monitor.chat("Tell me about AI")
```

---

## üí° Tips Cu·ªëi C√πng

1. **Lu√¥n d√πng `gemini-2.5-flash`** tr·ª´ khi c·∫ßn reasoning ph·ª©c t·∫°p
2. **Gi·ªØ messages ng·∫Øn g·ªçn** - kh√¥ng g·ª≠i qu√° nhi·ªÅu history
3. **C·∫≠p nh·∫≠t cookies th∆∞·ªùng xuy√™n** - tr√°nh b·ªã rate limit
4. **S·ª≠ d·ª•ng streaming** cho response d√†i
5. **Ch·∫°y production mode** v·ªõi multiple workers
6. **Monitor performance** ƒë·ªÉ ph√°t hi·ªán v·∫•n ƒë·ªÅ s·ªõm

---

**Ch√∫c b·∫°n c√≥ API si√™u nhanh! ‚ö°üöÄ**
